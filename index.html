<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PointRefer">
  <meta name="keywords" content="PointRefer">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PointRefer</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">PointRefer: Grasp the Inter-class Relations for 3D Visual Grounding with Referred Point Selection</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=CEEtEnoAAAAJ&hl=zh-CN">Hao Liu</a><sup>1</sup> &nbsp;&nbsp;&nbsp;
              <a href="https://scholar.google.com/citations?user=jc58aTgAAAAJ&hl=zh-CN">Yanni Ma</a><sup>2</sup> &nbsp;&nbsp;&nbsp;
              <a href="https://scholar.google.com/citations?user=WQRNvdsAAAAJ&hl=zh-CN">Yulan Guo</a><sup>2</sup> &nbsp;&nbsp;&nbsp;
              <a href="https://scholar.google.com/citations?user=ISNmBxwAAAAJ&hl=zh-CN">Ying He</a><sup>1</sup> &nbsp;&nbsp;&nbsp;
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Nanyang Technological University</span> &nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>Sun Yat-Sen University</span> &nbsp;&nbsp;&nbsp;<br>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-justified">
            <b>TL;DR:</b> We propose a point-based single-stage method PointRefer, designed to grasp the inter-class object relations implied in the language description to distinguish the target object from similar objects of the same category.
      </h2>
    </div>
  </div>
</section>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PointRefer Video</title>
    <style>
        /* 视频居中和调整大小 */
        .video-container {
            display: flex; /* 使用 Flexbox 布局 */
            justify-content: center; /* 水平居中 */
            align-items: center; /* 垂直居中 */
            flex-direction: column; /* 垂直排列文本和视频 */
            height: 100vh; /* 视口高度 */
            background-color: #f9f9f9; /* 背景颜色（可选） */
        }

        video {
            width: 90%; /* 视频宽度占容器的 90% */
            max-width: 800px; /* 设置最大宽度 */
            border: 2px solid #ccc; /* 边框 */
            border-radius: 10px; /* 圆角 */
            box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2); /* 阴影 */
        }
    </style>
</head>
<body>
    <section class="video-container">
        <video controls>
            <source src="file/demo.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </section>
</body>
</html>

<section class="section" >
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            3D visual grounding aims to locate a target object within point clouds based on a free-form language description. Existing methods typically extract sentence-level features by coupling all words together and use a language-to-object classifier to predict the target category. Consequently, these methods focus more on object categories while neglecting auxiliary objects and inter-class spatial relations in the query text. Additionally, these methods struggle to localize the referential target when similar objects are present. 
            To address these issues, we propose a point-based single-stage method PointRefer, designed to grasp the inter-class object relations implied in the language description. PointRefer decouples textual entities in the input text and learns the spatial relations between entities (i.e., the main object and auxiliary objects) to accurately locate the referential target. Specifically, we first introduce a text parsing module that generates textual features for the Main object and Auxiliary object components in the input text. Then, we propose a language-aware sampling module, which selects a set of keypoints and auxiliary points based on the textual features of these parsed components. Finally, we develop a relation-aware transformer module that focuses on the target object's points. This module models the relationships between any two keypoints and auxiliary points, and then employs cross-modality attention layers to  refine keypoint selection in a coarse-to-fine manner. Extensive experiments show that our PointRefer achieves comparable or superior performance to existing methods on both the ScanRefer and Nr3D/Sr3D datasets.
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section" style="background-color:#efeff081">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Contribution</h2>
        <div class="content has-text-justified">
          <p>
            Our main contributions are
            </p><ol type="1">
              <li><span style="font-size: 95%;">We propose a point-based single-stage method, PointRefer, for 3D VG, which explicitly utilizes the inter-class relationship implied in natural language descriptions to locate the referential target. </span></li>
              <li><span style="font-size: 95%;">We propose a relation-aware transformer, which captures the complex object relations through distance encoding and progressively refines target-related keypoints by aligning language features using cross-modality attention.</span></li>
              <li><span style="font-size: 95%;">Extensive experiments on the ScanRefer and Nr3D/Sr3D datasets demonstrate the strong performance of our PointRefer, particularly in scenes where similar objects of the same category are present.</span></li>
            </ol>  
          <p></p>

        </div>
      </div>
    </div>      
  </div>
</section>

<section class="section" >
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3"> <img id="painting_icon" width="4%" src="https://img.icons8.com/?size=1x&id=oOOSYZyuA844&format=png"> PointRefer Model</h2>
        <div class="content has-text-justified">
          <p>
          </p>
          PointRefer is a point-based single-stage medel for 3D visual grounding. PointRefer takes a pair of scene point cloud and sentence as input. The input text is parsed into Main object and Auxiliary object components. The input point clouds are fed into a pre-trained PointNet++ network to extract the seed points $P_{seed}$. Then, we use the language-aware down-sampling module to extract keypoints $P_{main}$ and auxiliary points $P_{auxi}$ from the seed points $P_{seed}$. Finally, we use relation-aware Transformer layers to grasp inter-class relations and align keypoint features with sentence-level features to localize the referential target.
        </div>
        <img id="painting_icon" width="100%" src="file/model.PNG">
      </div>
    </div>
</section>

<section class="section" >
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3"> <img id="painting_icon" width="4%" src="https://img.icons8.com/?size=1x&id=sei9JmRv5YVb&format=png"> Visualization on ScanRefer </h2>
        <div class="content has-text-justified">
          <p>
            Qualitative results on ScanRefer. Compared with 3D-SPS, PointRefer can distinguish the target object from similar objects, with the assistance of auxiliary object.
          </p>
        </div>
        <img id="painting_icon" width="100%" src="file/qualititive_results_1.PNG">
        <div class="content has-text-justified">
          <p>
            Qualitative results with rare object categories on ScanRefer. Compared with 3D-SPS, PointRefer has a superior understanding of spatial relationship, enabling the identification of objects with rare categories.
          </p>
        </div>
        <img id="painting_icon" width="100%" src="file/qualititive_results_2.PNG">
      </div>
    </div>
</section>

<section class="section" >
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3"> <img id="painting_icon" width="4%" src="https://img.icons8.com/?size=1x&id=sei9JmRv5YVb&format=png"> Examples on 3DVL tasks </h2>
        <div class="content has-text-justified">
          <p>
            Failure cases on ScanRefer. The first and second columns: Failures due to ambiguous descriptions. The third column: Failures caused by complex descriptions.
          </p>s
        </div>
        <img id="painting_icon" width="100%" src="file/failure_cases.PNG">
      </div>
    </div>
</section>

</body>
</html>
