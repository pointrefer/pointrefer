<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PointRefer">
  <meta name="keywords" content="PointRefer">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PointRefer</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">PointRefer: Grasping Inter-class Relations for 3D Visual Grounding via Language-Guided Point Selection</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=CEEtEnoAAAAJ&hl=zh-CN">Hao Liu</a><sup>1</sup> &nbsp;&nbsp;&nbsp;
              <a href="https://scholar.google.com/citations?user=jc58aTgAAAAJ&hl=zh-CN">Yanni Ma</a><sup>2</sup> &nbsp;&nbsp;&nbsp;
              <a href="https://scholar.google.com/citations?user=WQRNvdsAAAAJ&hl=zh-CN">Yulan Guo</a><sup>2</sup> &nbsp;&nbsp;&nbsp;
              <a href="https://scholar.google.com/citations?user=ISNmBxwAAAAJ&hl=zh-CN">Ying He</a><sup>1</sup> &nbsp;&nbsp;&nbsp;
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Nanyang Technological University</span> &nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>Sun Yat-Sen University</span> &nbsp;&nbsp;&nbsp;<br>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-justified">
            <b>TL;DR:</b>We propose a point-based single-stage method PointRefer, which captures the inter-class object relations implied in the language description to distinguish the target object from similar objects.
      </h2>
    </div>
  </div>
</section>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=0.8">
    <title>PointRefer Video</title>
    <style>
        /* 视频居中和调整大小 */
        .video-container {
            display: flex; /* 使用 Flexbox 布局 */
            justify-content: center; /* 水平居中 */
            align-items: center; /* 垂直居中 */
            flex-direction: column; /* 垂直排列文本和视频 */
            height: 60vh; /* 视口高度 */
            background-color: #f9f9f9; /* 背景颜色（可选） */
        }

        video {
            width: 100%; /* 视频宽度占容器的 100% */
            max-width: 800px; /* 设置最大宽度 */
            border: 2px solid #ccc; /* 边框 */
            border-radius: 10px; /* 圆角 */
            box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2); /* 阴影 */
        }
    </style>
</head>
<body>
    <section class="video-container" style="margin-top: -120px;">
        <video controls>
            <source src="file/demo.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </section>
</body>
</html>

<section class="section" >
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            3D visual grounding aims to locate a target object within point clouds based on a free-form language description. However, existing methods struggle to localize the referential target when similar objects 
            are present. To address this challenge, we propose a point-based single-stage method PointRefer, which captures the inter-class object relations implied in the language description to distinguish 
            the target object from similar objects. Inspired by the human perceptual system, PointRefer leverages neighboring (also referred to as auxiliary) objects to refine referential target localization and 
            reduce interference caused by similar objects. Specifically, it decouples textual entities in the input text and learns the spatial relations between entities (i.e., the main object and auxiliary
            objects) to accurately locate the referential target. First, we introduce a text parsing module to extract distinct textual features for the Main object and Auxiliary object components in 
            the input text. Then, a language-aware sampling module is proposed to select a set of keypoints and auxiliary points based on the textual features of these parsed components. Next, we develop a relation-aware
            transformer module that focuses on the target object's points. This module models the relationships between any two keypoints and auxiliary points, and then employs cross-modality attention layers to refine 
            keypoint selection in a coarse-to-fine manner. Finally, we predict visual grounding scores and regress 3D boxes on final keypoints. Extensive experiments show that our PointRefer achieves comparable or 
            superior performance to existing methods on both the ScanRefer and NR3D/SR3D datasets. 
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section" style="background-color:#efeff081">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Contribution</h2>
        <div class="content has-text-justified">
          <p>
            Our main contributions are
            </p><ol type="1">
              <li><span style="font-size: 95%;">We propose a point-based single-stage method, PointRefer, for 3D VG, which explicitly utilizes the inter-class relationship implied in natural language descriptions to 
                distinguish the referential target from similar objects. </span></li>
              <li><span style="font-size: 95%;">We propose a relation-aware transformer, which captures the complex object relations through distance encoding and progressively refines target-related keypoints by 
                aligning language features with cross-modality attention.</span></li>
              <li><span style="font-size: 95%;">Extensive experiments on the ScanRefer and NR3D/SR3D datasets demonstrate the strong performance of our PointRefer, particularly in scenes where similar objects of the 
                same category are present.</span></li>
            </ol>  
          <p></p>

        </div>
      </div>
    </div>      
  </div>
</section>

<section class="section" >
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3"> <img id="painting_icon" width="4%" src="https://img.icons8.com/?size=1x&id=oOOSYZyuA844&format=png"> PointRefer Model</h2>
        <div class="content has-text-justified">
          <p>
          </p>
          PointRefer is a point-based single-stage medel for 3D visual grounding. PointRefer takes a pair of scene point cloud and sentence as input. The input text is parsed into Main object and Auxiliary object 
          components. The input point clouds are fed into a pre-trained PointNet++ network to extract the seed points $P_{seed}$. Then, we use the language-aware down-sampling module to extract keypoints $P_{main}$
          and auxiliary points $P_{auxi}$ from the seed points $P_{seed}$. Finally, we use relation-aware Transformer layers to grasp inter-class relations and align keypoint features with sentence-level features 
          to localize the referential target.
        </div>
        <img id="painting_icon" width="100%" src="file/model.PNG">
      </div>
    </div>
</section>

<section class="section" >
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3" style="margin-bottom: 20px;"> <img id="painting_icon" width="4%" src="https://img.icons8.com/?size=1x&id=sei9JmRv5YVb&format=png"> Visualization on ScanRefer </h2>
        <div class="content has-text-justified">
          <p>
            Qualitative results on ScanRefer. Compared with 3D-SPS, PointRefer can distinguish the target object from similar objects, with the assistance of auxiliary object. 3D-SPS and PointRefer are both equipped 
            the same VoteNet-like backbone.
          </p>
        </div>
        <img id="painting_icon" width="100%" src="file/qualitative_results_1.PNG" style="margin-bottom: 40px;">
        <div class="content has-text-justified">
          <p>
            Qualitative results with rare object categories on ScanRefer. Compared with 3D-SPS, PointRefer has a superior understanding of spatial relationship, enabling the identification of objects with rare categories.
          </p>
        </div>
        <img id="painting_icon" width="100%" src="file/qualitative_results_2.PNG">
      </div>
    </div>
</section>

<section class="section" >
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3"> <img id="painting_icon" width="4%" src="https://img.icons8.com/?size=1x&id=sei9JmRv5YVb&format=png"> Failure Cases on ScanRefer </h2>
        <div class="content has-text-justified">
          <p>
            Failure cases on ScanRefer. The first and second columns: Failures due to ambiguous descriptions, which are introduced by the dataset itself.
            The third column: Failures caused by complex descriptions.
          </p>
        </div>
        <img id="painting_icon" width="100%" src="file/failure_cases.PNG">
      </div>
    </div>
</section>

</body>
</html>
